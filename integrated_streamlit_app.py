
# ----- Start of Streamlit App -----

import streamlit as st
import pandas as pd

# ----- Imports from Original Code -----
import streamlit as st
from google.colab import files
import os
import zipfile
import pandas as pd
import pandasql as psql
import openai
# --------------------------------------

# ----- Data Fetching from Original Code (Commented Out) -----
# Comment: The following code is specific to Colab and Kaggle, hence commented out.
# # Upload kaggle.json
# files.upload()
# !mkdir -p ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
# !pip install kaggle
# !kaggle datasets download -d danielgrijalvas/movies
# !kaggle datasets download -d claymaker/us-largest-companies
# !kaggle datasets download -d andrewmvd/music-sales
# ----------------------------------------------------------

# Function to load data (assuming it's pre-downloaded)
def load_data():
    # Load your data here, e.g., using pandas
    return pd.read_csv('your_data.csv')

# Initialize Streamlit app
def main():
    st.title('Your Streamlit Chat App')
    data = load_data()

    # Create chat interface
    user_input = st.text_input('Type your question here:')
    
    if st.button('Submit'):
        # Your existing logic to process the user_input and fetch result from data
        st.write('Response from model')

    # ----- Main Logic from Original Code -----
    # Comment: The following is the main logic from your original code.
    # -*- coding: utf-8 -*-
"""tabular_data_llm_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y47fEgrY38YeKtBGAoweUZGQLIOc_K7G
"""

!pip install streamlit

"""Set up Kaggle API"""




"""Download data. Using relatively small but wide breadth of data to determine efficacy of decision making via LLM in user query ingestion."""


"""Take downloaded files and unzip into folders"""


# Create directories for each dataset
dataset_names = ['movies', 'companies', 'music']
for name in dataset_names:
    os.makedirs(name, exist_ok=True)

# Unzip datasets into their respective directories
with zipfile.ZipFile("movies.zip", 'r') as zip_ref:
    zip_ref.extractall("movies/")

with zipfile.ZipFile("us-largest-companies.zip", 'r') as zip_ref:
    zip_ref.extractall("companies/")

with zipfile.ZipFile("music-sales.zip", 'r') as zip_ref:
    zip_ref.extractall("music/")

"""Read data into pandas dataframe

"""


movies_df = pd.read_csv('movies/movies.csv')
companies_df = pd.read_csv('companies/webscrape.csv')
music_df = pd.read_csv('music/musicdata.csv')

companies_df.rename(columns={'Revenue (USD millions)': 'Revenue'}, inplace=True)

"""EDA"""

# Display the first few rows
print("Companies Data:")
print(companies_df.head())

"""Let's use pandasql to show the sql skills:"""

!pip install pandasql

b= psql.sqldf("SELECT * FROM movies_df LIMIT 5;", locals())
c=psql.sqldf("SELECT * FROM music_df LIMIT 5;", locals())
psql.sqldf('''SELECT * FROM companies_df LIMIT 5;''')

"""Connect to openai"""

!pip install openai

with open('/content/drive/MyDrive/Adult life /Projects/llm_key.txt', 'r') as file:
    api_key = file.read()

openai.api_key = api_key

"""Get some metadata about data we downloaded"""

# Assume companies_df, movies_df, and music_df are your dataframes
dataframes = {'companies_df': companies_df, 'movies_df': movies_df, 'music_df': music_df}

# Initialize an empty string to hold our output data
df_metadata = ""

for df_name, df in dataframes.items():
    df_metadata += f"Dataframe: {df_name}\n"
    df_metadata += "Column Names and Types:\n"

    # Iterate over columns and data types
    for col, dtype in zip(df.columns, df.dtypes):
        df_metadata += f"  - {col}: {dtype}\n"

    df_metadata += "\n"  # Separate entries for readability

# Now, df_metadata contains the information you wanted in a text format
print(df_metadata)

"""Please provide input for a query about the datasets we have:

"""

user_query = input("Please enter your query: ")  # Replace this with Streamlit or other UI method to get user input

"""generate the sql to query the dataframes and get more constrained and relevant data."""

def generate_sql_query(context, prompt):
    response = openai.Completion.create(
      engine="text-davinci-002",
      prompt=f"{context}\n\nUser Query: {prompt}\n\nSQL Query:",
      max_tokens=100,

    )
    print(prompt)
    print(response)
    return response.choices[0].text.strip()

context_for_sql = f"{df_metadata}\nUse like and wildcards on the where clauses. Have the sql show the most columns it can related to the user's query. Have the sql provide you flexibility showing more data than less. "
sql_query = generate_sql_query(context_for_sql, user_query)

def extract_table_from_sql(sql_query):
    # Convert to uppercase to make it case-insensitive
    sql_query_up = sql_query.upper()

    # Find the starting index of 'FROM ' substring
    from_index = sql_query_up.index('FROM ') + 5

    # Extract the string after 'FROM '
    tail_str = sql_query_up[from_index:]

    # Find the first space, or the end of string, to extract the table name
    space_index = tail_str.find('\n')
    if space_index == -1:  # Handle case where 'FROM' is the last clause
        space_index = len(tail_str)

    # Extract and return table name
    return tail_str[:space_index].strip()


#table_in_query = extract_table_from_sql(sql_query)
#print(f"Table in query: {table_in_query}")

"""run said sql on dfs"""

def execute_sql_query(sql_query, df_dict):
    # Extract the table name from the SQL query
    table_in_query = extract_table_from_sql(sql_query).lower()  # Convert to lowercase

    # Convert df_dict keys to lowercase for case-insensitive comparison
    lower_df_dict = {k.lower(): v for k, v in df_dict.items()}

    # Check if the table name exists in the df_dict
    if table_in_query in lower_df_dict:
        # Get the actual dataframe from the dictionary
        target_df = lower_df_dict[table_in_query]

        # Execute the query on the target dataframe
        result_df = psql.sqldf(sql_query)
        return result_df
    else:
        return f"Table {table_in_query} not found."

# Dataframe dictionary, assuming the actual dataframes are initialized
df_dict = {'companies_df': companies_df, 'movies_df': movies_df, 'music_df': music_df}

# Example usage
#sql_query = "SELECT * FROM companies_df WHERE id = 1"
sql_result = execute_sql_query(sql_query, df_dict)
print(result)

"""convert the dataframe output into markdown for LLM ingestion"""

def df_to_markdown(df):
    return "```markdown\n" + df.to_markdown() + "\n```"

result_markdown = df_to_markdown(sql_result)

"""generate the final answer for the user"""

def generate_final_answer(context, prompt):
    response = openai.Completion.create(
      engine="text-davinci-002",
      prompt=f"{context}\n\nSQL Query Result:\n{result_markdown}\n\nUser Query: {prompt}\n\nAnswer:",
      max_tokens=200
    )
    print(response)
    return response.choices[0].text.strip()

final_context = f"{df_metadata}\nBased on this data and context, answer the user query. Provide as much data context as possible."
final_answer = generate_final_answer(final_context, user_query)

final_answer

final_answer
    # ----------------------------------------

# Run the app
if __name__ == '__main__':
    main()
